word_counts = rule('s3://AKIAIOV23F6ZNL5YPRNA:8Gwz48zgzwoYIZv70V4uGDD6%2fdNtHdbFq4kLXGlR@aws-publicdatasets/common-crawl/crawl-002/', 
              'word_counts')

word_counts.param('maxinput', 1)

#@word_counts.map
#def map(record, params):
#  for word in record['payload'].split():
#    if '\xff' in word or '\x00' in word: return
#    yield str(word.decode('ascii', 'ignore')), 1
    

import re    
@word_counts.map
def map(doc, params,  tokenizer = re.compile('[^A-Z0-9_.]+', flags=re.I)):
  for word in tokenizer.split(doc['payload']):
    yield word, 1

@word_counts.reduce
def reduce(iter, params):
  from disco.util import kvgroup
  for word, counts in kvgroup(sorted(iter)):
    yield word, sum(counts)